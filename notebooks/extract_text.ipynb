{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from config import DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_with_delimiter(text: str, delim: str) -> list:\n",
    "    split_sentences = re.split(f\"({delim})\", text)\n",
    "    combined_sentences: list = []\n",
    "    for i in range(0, len(split_sentences)):\n",
    "        if split_sentences[i] == \".\":\n",
    "            combined_sentences[-1] += split_sentences[i]\n",
    "        else:\n",
    "            combined_sentences.append(split_sentences[i])\n",
    "    return combined_sentences\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    clean_text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    clean_text = re.sub(r\" {2,}\", \" \", clean_text)\n",
    "    clean_text = re.sub(r\" \\n\", \"\\n\", clean_text)\n",
    "    clean_text = re.sub(r\"\\n \", \"\\n\", clean_text).strip()\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def extract_text_from_html(\n",
    "    book: str,\n",
    "    book_name: str,\n",
    "    max_sentence_length: int = 100,\n",
    ") -> None:\n",
    "    file = DATA / \"unzipped\" / book / \"index.html\"\n",
    "    output_folder = DATA / \"processed\" / book\n",
    "\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(file, \"r\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    data = []\n",
    "    excluded_sections = [\"GLOSSARY\", \"NOTES\", \"APPENDIX\", \"INTRODUCTION\"]\n",
    "\n",
    "    for section in soup.select(\"section\"):\n",
    "        if (\n",
    "            section.find(\"h2\")\n",
    "            and section.find(\"h2\").get_text().strip() not in excluded_sections\n",
    "        ):\n",
    "            section_title = section.find(\"h2\").get_text()\n",
    "            section_text = \"\"\n",
    "\n",
    "            for t in section.select(\"p\"):\n",
    "                for elem_to_remove in (\n",
    "                    t.select(\"[class='calibre24']\")\n",
    "                    + t.select(\"[class='mw-ref']\")\n",
    "                    + t.select(\"[class='reference']\")\n",
    "                ):\n",
    "                    elem_to_remove.decompose()\n",
    "                section_text += \"\\n\" + t.get_text()\n",
    "\n",
    "            section_text = clean_text(section_text)\n",
    "\n",
    "            fixed_length_sentences = []\n",
    "\n",
    "            for paragraph in section_text.split(\"\\n\"):\n",
    "                if len(paragraph.split()) > max_sentence_length:\n",
    "                    sentences = split_with_delimiter(paragraph, \"\\.\")\n",
    "                    current_sentence = \"\"\n",
    "\n",
    "                    for i in range(len(sentences)):\n",
    "                        if (\n",
    "                            len(current_sentence.split()) + len(sentences[i].split())\n",
    "                            < max_sentence_length\n",
    "                        ):\n",
    "                            current_sentence += sentences[i]\n",
    "                        else:\n",
    "                            fixed_length_sentences.append(current_sentence)\n",
    "                            current_sentence = sentences[i]\n",
    "                else:\n",
    "                    fixed_length_sentences.append(paragraph)\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"title\": section_title,\n",
    "                    \"url\": f\"https://en.wikisource.org/wiki/{book}#{'_'.join(section_title.split())}\",\n",
    "                    \"sentences\": fixed_length_sentences,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    output = {\n",
    "        \"book_title\": book_name,\n",
    "        \"url\": f\"https://en.wikisource.org/wiki/{book}\",\n",
    "        \"data\": data,\n",
    "    }\n",
    "\n",
    "    json.dump(output, open(output_folder / f\"{book}.json\", \"w\"), indent=4)\n",
    "    print(f\"Saved {book}.json with content of book.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe.json with content of book.\n"
     ]
    }
   ],
   "source": [
    "extract_text_from_html(\n",
    "    \"Marcus_Aurelius_Antoninus_-_His_Meditations_concerning_himselfe\",\n",
    "    \"Meditations by Marcus Aurelius\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f7d7ce7694bb4f4c294d506e5b6dc7957106f5332d820f0757e3d8cd7b1bbf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
